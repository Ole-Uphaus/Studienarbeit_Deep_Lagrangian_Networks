{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f43c0c",
   "metadata": {},
   "source": [
    "In diesem Notebook möchte ich die autograd funktion von pytorch anhand eines kleinen Netzwerkes testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c54cb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebebca",
   "metadata": {},
   "source": [
    "Erstellung des einfachen Netzwerkes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1258cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Schichten festlegen\n",
    "        self.Layer_1 = nn.Linear(3, 16)\n",
    "        self.Output_Layer = nn.Linear(16, 2)\n",
    "\n",
    "        # Aktivierungsfunktion definieren\n",
    "        self.activation_fnc = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer_1(x)\n",
    "        x = self.activation_fnc(x)\n",
    "        x = self.Output_Layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "test_network = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447336eb",
   "metadata": {},
   "source": [
    "Ein paar einfache werte austesten (mit Gradientenverfolgung des Eingangs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a02779f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Input: \n",
      " tensor([[1.0000, 2.0000, 1.5000],\n",
      "        [1.3000, 2.5000, 1.1000]], requires_grad=True)\n",
      "Output: \n",
      " tensor([[0.7466, 0.3899],\n",
      "        [0.8794, 0.4927]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.tensor([[1, 2, 1.5], [1.3, 2.5, 1.1]], dtype=torch.float32, requires_grad=True)\n",
    "print('Test Input: \\n', test_input)\n",
    "\n",
    "test_output = test_network(test_input)\n",
    "print('Output: \\n', test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27673e79",
   "metadata": {},
   "source": [
    "Gradienten bezüglich der Eingangsgrößen berechnen (man kann den Gradienten nur bezüglich einer skalaren Ausgangsgröße berechnen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobimatrix berechnen: \n",
      " tensor([[-0.0858,  0.2688,  0.0785],\n",
      "        [ 0.1003,  0.0643, -0.0129]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "J = jacobian(lambda inp: test_network(inp), test_input[0], create_graph=True)  # lambda ist in diesem fall einfach eine kleine Funktion (nur nicht über def), da jacobian eine Funktion als eingabe erwartet. Die Funktion wird dann mit dem test_input ausgewertet und der Gradient wird erstellt.\n",
    "print('Jacobimatrix berechnen: \\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a556f964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechnung Gradient 1 mit Grad: \n",
      " (tensor([[-0.0858,  0.2688,  0.0785],\n",
      "        [ 0.0272,  0.3049, -0.0486]], grad_fn=<MmBackward0>),)\n",
      "Berechnung Gradient 2 mit Grad: \n",
      " (tensor([[ 0.1003,  0.0643, -0.0129],\n",
      "        [ 0.1722,  0.0873, -0.0938]], grad_fn=<MmBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "gradient_1 = grad(test_output[:, 0].sum(), test_input, create_graph=True)\n",
    "gradient_2 = grad(test_output[:, 1].sum(), test_input, create_graph=True)\n",
    "\n",
    "print('Berechnung Gradient 1 mit Grad: \\n', gradient_1)\n",
    "print('Berechnung Gradient 2 mit Grad: \\n', gradient_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448d82e",
   "metadata": {},
   "source": [
    "Prüfen, ob die backward() funktion auch auf Basis der Jacobimatrix funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00957f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: \n",
      " tensor(0.4133, grad_fn=<SumBackward0>)\n",
      "\n",
      "Gradient des Test Loss nach eingängen: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Layer_1.weight : \n",
      " tensor([[ 0.2019,  0.2019,  0.2019],\n",
      "        [-0.1288, -0.1288, -0.1288],\n",
      "        [-0.0281, -0.0281, -0.0281],\n",
      "        [ 0.1557,  0.1557,  0.1557],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2105,  0.2105,  0.2105],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1123,  0.1123,  0.1123],\n",
      "        [ 0.1484,  0.1484,  0.1484],\n",
      "        [-0.2207, -0.2207, -0.2207],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2296,  0.2296,  0.2296],\n",
      "        [ 0.3434,  0.3434,  0.3434]])\n",
      "\n",
      "Layer_1.bias : \n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "test_loss = J.sum()\n",
    "print('Test Loss: \\n', test_loss)\n",
    "print()\n",
    "\n",
    "test_loss.backward()\n",
    "print('Gradient des Test Loss nach eingängen: \\n', test_input.grad)\n",
    "print()\n",
    "\n",
    "name_param = list(test_network.named_parameters())\n",
    "print(name_param[0][0], ': \\n', name_param[0][1].grad)\n",
    "print()\n",
    "print(name_param[1][0], ': \\n', name_param[1][1].grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10375e75",
   "metadata": {},
   "source": [
    "Anmerkung: Wenn keine aktivierungsfunktion verwendet wird, passiert folgendes:\n",
    "\n",
    "-Gradient des Test Loss bezüglich der Eingänge ist None\n",
    "    \n",
    "-Gradient des bias bezüglich der Eingänge ist None\n",
    "\n",
    "Das liegt daran, dass diese Einträge beim Ableiten herausfliegen. (hiernach fehlt etwas - siehe chatgpt oder andere skripte)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Studienarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
