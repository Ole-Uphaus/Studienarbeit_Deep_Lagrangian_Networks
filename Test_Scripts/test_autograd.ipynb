{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f43c0c",
   "metadata": {},
   "source": [
    "In diesem Notebook möchte ich die autograd funktion von pytorch anhand eines kleinen Netzwerkes testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54cb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebebca",
   "metadata": {},
   "source": [
    "Erstellung des einfachen Netzwerkes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1258cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Schichten festlegen\n",
    "        self.Layer_1 = nn.Linear(3, 16)\n",
    "        self.Output_Layer = nn.Linear(16, 2)\n",
    "\n",
    "        # Aktivierungsfunktion definieren\n",
    "        self.activation_fnc = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer_1(x)\n",
    "        x = self.activation_fnc(x)\n",
    "        x = self.Output_Layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "test_network = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447336eb",
   "metadata": {},
   "source": [
    "Ein paar einfache werte austesten (mit Gradientenverfolgung des Eingangs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02779f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Input: \n",
      " tensor([1.0000, 2.0000, 1.5000], requires_grad=True)\n",
      "Ergebnis: \n",
      " tensor([0.8292, 0.1727], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.tensor([1, 2, 1.5], dtype=torch.float32, requires_grad=True)\n",
    "print('Test Input: \\n', test_input)\n",
    "\n",
    "test_output = test_network(test_input)\n",
    "print('Ergebnis: \\n', test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27673e79",
   "metadata": {},
   "source": [
    "Gradienten bezüglich der Eingangsgrößen berechnen (man kann den Gradienten nur bezüglich einer skalaren Ausgangsgröße berechnen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e44868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobimatrix berechnen: \n",
      " tensor([[ 0.1909,  0.3073,  0.0489],\n",
      "        [-0.0410,  0.0945,  0.1398]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "J = jacobian(lambda inp: test_network(inp), test_input, create_graph=True)  # lambda ist in diesem fall einfach eine kleine Funktion (nur nicht über def), da jacobian eine Funktion als eingabe erwartet. Die Funktion wird dann mit dem test_input ausgewertet und der Gradient wird erstellt.\n",
    "print('Jacobimatrix berechnen: \\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a556f964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechnung Gradient mit Grad: \n",
      " (tensor([0.1499, 0.4018, 0.1887], grad_fn=<ViewBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "gradient = grad(test_output, test_input, grad_outputs=torch.ones_like(test_output), create_graph=True)\n",
    "\n",
    "print('Berechnung Gradient mit Grad: \\n', gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448d82e",
   "metadata": {},
   "source": [
    "Prüfen, ob die backward() funktion auch auf Basis der Jacobimatrix funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00957f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: \n",
      " tensor(0.7404, grad_fn=<SumBackward0>)\n",
      "\n",
      "Gradient des Test Loss nach eingängen: \n",
      " tensor([0., 0., 0.])\n",
      "\n",
      "Layer_1.weight : \n",
      " tensor([[ 0.1466,  0.1466,  0.1466],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0313,  0.0313,  0.0313],\n",
      "        [ 0.1104,  0.1104,  0.1104],\n",
      "        [ 0.1408,  0.1408,  0.1408],\n",
      "        [-0.0292, -0.0292, -0.0292],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1505,  0.1505,  0.1505],\n",
      "        [ 0.2284,  0.2284,  0.2284],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4545,  0.4545,  0.4545],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n",
      "\n",
      "Layer_1.bias : \n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "test_loss = J.sum()\n",
    "print('Test Loss: \\n', test_loss)\n",
    "print()\n",
    "\n",
    "test_loss.backward()\n",
    "print('Gradient des Test Loss nach eingängen: \\n', test_input.grad)\n",
    "print()\n",
    "\n",
    "name_param = list(test_network.named_parameters())\n",
    "print(name_param[0][0], ': \\n', name_param[0][1].grad)\n",
    "print()\n",
    "print(name_param[1][0], ': \\n', name_param[1][1].grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10375e75",
   "metadata": {},
   "source": [
    "Anmerkung: Wenn keine aktivierungsfunktion verwendet wird, passiert folgendes:\n",
    "\n",
    "-Gradient des Test Loss bezüglich der Eingänge ist None\n",
    "    \n",
    "-Gradient des bias bezüglich der Eingänge ist None\n",
    "\n",
    "Das liegt daran, dass diese Einträge beim Ableiten herausfliegen. (hiernach fehlt etwas - siehe chatgpt oder andere skripte)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Studienarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
